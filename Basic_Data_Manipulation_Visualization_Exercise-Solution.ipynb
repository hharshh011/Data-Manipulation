#!/usr/bin/env python
# coding: utf-8

# # Basic Exercises on Data Importing - Understanding - Manipulating - Analysis - Visualization

# ## Section-1: The pupose of the below exercises (1-7) is to create dictionary and convert into dataframes, how to diplay etc...
# ## The below exercises required to create data 

# ### 1. Import the necessary libraries (pandas, numpy, datetime, re etc)

# In[67]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime as dt
import seaborn as sns
import re

# set the graphs to show in the jupyter notebook
get_ipython().run_line_magic('matplotlib', 'inline')

# set seabor graphs to a better style
sns.set(style="ticks")


# ### 2. Run the below line of code to create a dictionary and this will be used for below exercises

# In[2]:


raw_data = {"name": ['Bulbasaur', 'Charmander','Squirtle','Caterpie'],
            "evolution": ['Ivysaur','Charmeleon','Wartortle','Metapod'],
            "type": ['grass', 'fire', 'water', 'bug'],
            "hp": [45, 39, 44, 45],
            "pokedex": ['yes', 'no','yes','no']                        
            }


# ### 3. Assign it to a object called pokemon and it should be a pandas DataFrame

# In[4]:


pokemon = pd.DataFrame.from_dict(raw_data) 


# In[5]:


pokemon


# ### 4. If the DataFrame columns are in alphabetical order, change the order of the columns as name, type, hp, evolution, pokedex

# In[9]:


pokemon.set_index(["name","type","hp","evolution","pokedex"],inplace = False)


# ### 5. Add another column called place, and insert places (lakes, parks, hills, forest etc) of your choice.

# In[16]:


pokemon['place']=['forest','hills','lakes','parks']   


# In[17]:


pokemon


# ### 6. Display the data type of each column

# In[20]:


pokemon.dtypes


# ### 7. Display the info of dataframe

# In[23]:


pokemon.info()


# ## Section-2: The pupose of the below exercise (8-20) is to understand deleting data with pandas.
# ## The below exercises required to use wine.data

# ### 8. Import the dataset *wine.txt* from the folder and assign it to a object called wine
# 
# Please note that the original data text file doesn't contain any header. Please ensure that when you import the data, you should use a suitable argument so as to avoid data getting imported as header.

# In[ ]:





# In[14]:


wine = pd.read_table("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//wine.txt",sep=",",header=None)


# In[15]:


wine


# ### 9. Delete the first, fourth, seventh, nineth, eleventh, thirteenth and fourteenth columns

# In[16]:


wine.drop(wine.columns[[0, 3, 6, 8, 10, 12, 13]], axis = 1, inplace = True)


# In[17]:


wine


# ### 10. Assign the columns as below:
# 
# The attributes are (dontated by Riccardo Leardi, riclea '@' anchem.unige.it):  
# 1) alcohol  
# 2) malic_acid  
# 3) alcalinity_of_ash  
# 4) magnesium  
# 5) flavanoids  
# 6) proanthocyanins  
# 7) hue 

# In[19]:


wine.columns = ["alcohol","malic_acid","alcalinity_of_ash","magnesium","flavanoids","proanthocyanins","hue"]


# In[21]:


wine.head()


# ### 11. Set the values of the first 3 values from alcohol column as NaN

# In[22]:


wine.alcohol.iloc[:3] = np.NAN


# In[23]:


wine.head()


# ### 12. Now set the value of the rows 3 and 4 of magnesium as NaN

# In[24]:


wine.magnesium.iloc[2:4] = np.NAN


# In[26]:


wine.head()


# ### 13. Fill the value of NaN with the number 10 in alcohol and 100 in magnesium

# In[27]:


wine.alcohol = wine.alcohol.fillna(10)
wine.magnesium = wine.magnesium.fillna(100)


# In[28]:


wine.head()


# ### 14. Count the number of missing values in all columns.

# In[32]:


wine.isnull().sum()


# ### 15.  Create an array of 10 random numbers up until 10 and save it.

# In[34]:


randNum = np.random.randint(0,11,10)


# In[35]:


randNum


# ### 16.  Set the rows corresponding to the random numbers to NaN in the column *alcohol*

# In[36]:


wine.alcohol.iloc[randNum] = np.nan


# In[38]:


wine.head()


# ### 17.  How many missing values do we have now?

# In[39]:


wine.isnull().sum()


# ### 18. Print only the non-null values in alcohol

# In[40]:


non_null_values = wine.alcohol.dropna()
non_null_values


# ### 19. Delete the rows that contain missing values

# In[41]:


wine = wine.dropna()
wine


# ### 20.  Reset the index, so it starts with 0 again

# In[42]:


wine.reset_index(drop=True)


# ## Section-3: The pupose of the below exercise (21-27) is to understand ***filtering & sorting*** data from dataframe.
# ## The below exercises required to use chipotle.tsv

# This time we are going to pull data directly from the internet.  
# Import the dataset directly from this link (https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv) and create dataframe called chipo

# In[47]:


chipo = pd.read_table("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//chipotle.tsv")


# In[48]:


chipo.head()


# ### 21. How many products cost more than $10.00? 
# 
# Use `str` attribute to remove the $ sign and convert the column to proper numeric type data before filtering.
# 

# In[49]:


chipo['item_price'] = chipo['item_price'].str.replace('$', '')
chipo['item_price'] = chipo['item_price'].astype(float)


# In[53]:


np.count_nonzero(chipo.item_price>10.00)


# In[52]:


chipo


# ### 22. Print the Chipo Dataframe & info about data frame

# In[59]:


chipo.info()


# ### 23. What is the price of each item? 
# - Delete the duplicates in item_name and quantity
# - Print a data frame with only two columns `item_name` and `item_price`
# - Sort the values from the most to less expensive

# In[60]:


chipo1 = chipo.groupby(by ="item_name")[["item_price"]].min().reset_index()
chipo1


# ### 24. Sort by the name of the item

# In[64]:


chipo.sort_values(by = "item_name",ascending=True).reset_index(drop=True)


# ### 25. What was the quantity of the most expensive item ordered?

# In[65]:


a = np.where(chipo.item_price == chipo.item_price.max(),chipo.quantity,0)
for i in a:
    if i!=0:
        print("Quantity of the most expensive item ordered is : ",i)


# ### 26. How many times were a Veggie Salad Bowl ordered?

# In[68]:


VSB = np.count_nonzero(chipo.item_name == "Veggie Salad Bowl")
print(VSB,"times a Veggie Salad Bowl was ordered.")


# ### 27. How many times people orderd more than one Canned Soda?

# In[69]:


CS = np.count_nonzero((chipo.item_name == "Canned Soda") & (chipo.quantity > 1))
print(CS,"times people orderd more than one Canned Soda.")


# ## Section-4: The purpose of the below exercises is to understand how to perform aggregations of data frame
# ## The below exercises (28-33) required to use occupation.csv

# ###  28. Import the dataset occupation.csv and assign object as users

# In[70]:


users = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//occupation.csv", sep="|", index_col="user_id")


# In[72]:


users.head()


# ### 29. Discover what is the mean age per occupation

# In[74]:


users.groupby('occupation').age.mean()


# ### 30. Discover the Male ratio per occupation and sort it from the most to the least.
# 
# Use numpy.where() to encode gender column.

# In[76]:


users['is_male'] = users.gender.apply(lambda x: True if x == 'M' else False)
users.is_male


# In[77]:


(users.groupby('occupation').is_male.sum() / users.groupby('occupation').gender.count()).sort_values(ascending = False)


# ### 31. For each occupation, calculate the minimum and maximum ages

# In[80]:


users.groupby('occupation').age.agg(['min', 'max'])


# ### 32. For each combination of occupation and gender, calculate the mean age

# In[81]:


users.groupby(['occupation', 'gender']).age.mean()


# ### 33.  For each occupation present the percentage of women and men

# In[82]:


# create a data frame and apply count to gender
gender_ocup = users.groupby(['occupation', 'gender']).agg({'gender': 'count'})

# create a DataFrame and apply count for each occupation
occup_count = users.groupby(['occupation']).count()

# divide the gender_ocup per the occup_count and multiply per 100
occup_gender = gender_ocup.div(occup_count, level = "occupation")
occup_gender.loc[:, 'gender']


# ## Section-6: The purpose of the below exercises is to understand how to use lambda-apply-functions
# ## The below exercises (34-41) required to use student-mat.csv and student-por.csv files 

# ### 34. Import the datasets *student-mat* and *student-por* and append them and assigned object as df

# In[89]:


af = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//student-mat.csv")


# In[90]:


bf = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//student-por.csv")


# In[91]:


af.head()


# In[92]:


bf.head()


# In[94]:


df = pd.concat([af,bf]).reset_index(drop=True)


# In[95]:


df.head()


# ### 35. For the purpose of this exercise slice the dataframe from 'school' until the 'guardian' column

# In[96]:


df = df.loc[:,:"guardian"]
df.head()


# ### 36. Create a lambda function that captalize strings (example: if we give at_home as input function and should give At_home as output.

# In[97]:


def Capital():
    return lambda x: x.capitalize()


# ### 37. Capitalize both Mjob and Fjob variables using above lamdba function

# In[98]:


df.Mjob = df.Mjob.apply(Capital())
df.Fjob = df.Fjob.apply(Capital())
df.head()


# ### 38. Print the last elements of the data set. (Last few records)

# In[99]:


df.tail(1)


# ### 39. Did you notice the original dataframe is still lowercase? Why is that? Fix it and captalize Mjob and Fjob.

# In[ ]:





# ### 40. Create a function called majority that return a boolean value to a new column called legal_drinker

# In[100]:


def majority(x):
    if x == 1:
        return True
    else:
        return False
df["legal_drinker"] = [majority(1) if x>=18 else majority(0) for x in df["age"]]


# In[103]:


df.head()


# ### 41. Multiply every number of the dataset by 10. 

# In[101]:


multiplier = df.apply(lambda x: x*10 if x.name in ['Medu', 'Fedu'] else x)


# In[102]:


multiplier


# ## Section-6: The purpose of the below exercises is to understand how to perform simple joins
# ## The below exercises (42-48) required to use cars1.csv and cars2.csv files 

# ### 42. Import the datasets cars1.csv and cars2.csv and assign names as cars1 and cars2

# In[105]:


cars1 = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//cars1.csv")


# In[106]:


cars2 = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//cars2.csv")


#    ### 43. Print the information to cars1 by applying below functions 
#    hint: Use different functions/methods like type(), head(), tail(), columns(), info(), dtypes(), index(), shape(), count(), size(), ndim(), axes(), describe(), memory_usage(), sort_values(), value_counts()
#    Also create profile report using pandas_profiling.Profile_Report

# In[134]:


cars1.info


# In[135]:


cars1.describe()


# ### 44. It seems our first dataset has some unnamed blank columns, fix cars1

# In[127]:


cars1 = cars1.dropna(axis = 1)


# ### 45. What is the number of observations in each dataset?

# In[132]:


print(cars1.shape[0])
print(cars2.shape[0])


# ### 46. Join cars1 and cars2 into a single DataFrame called cars

# In[128]:


cars = pd.concat([cars1, cars2])


# In[129]:


cars.head()


# ### 47. There is a column missing, called owners. Create a random number Series from 15,000 to 73,000.

# In[109]:


owners = pd.Series(np.random.randint(15000, 73000, cars.shape[0]))


# In[112]:


owners


# ### 48. Add the column owners to cars

# In[130]:


cars['owners'] = owners


# In[131]:


cars.head()


# ## Section-7: The purpose of the below exercises is to understand how to perform date time operations

# ### 49. Write a Python script to display the
# - a. Current date and time
# - b. Current year
# - c. Month of year
# - d. Week number of the year
# - e. Weekday of the week
# - f. Day of year
# - g. Day of the month
# - h. Day of week

# In[148]:


x = dt.datetime.now()
print("a. Current date and time: ",x)
print("b. Current year: ",x.year)
print("c. Month of year: ",x.month)
print("d. Week number of the year: ",x.isocalendar()[1])
print("e. Weekday of the week: ",x.strftime("%A"))
print("f. Day of year: ",x.timetuple().tm_yday)
print("g. Day of the month ",x.day)
print("h. Day of week: ",x.isocalendar()[2])


# ### 50. Write a Python program to convert a string to datetime.
# Sample String : Jul 1 2014 2:43PM 
# 
# Expected Output : 2014-07-01 14:43:00

# In[149]:


sample_date = dt.datetime.strptime('Jul 1 2014  2:43PM', '%b %d %Y %I:%M%p')
print(sample_date)


# ### 51. Write a Python program to subtract five days from current date.
# 
# Current Date : 2015-06-22
# 
# 5 days before Current Date : 2015-06-17

# In[150]:


y = x - dt.timedelta(5)
print(y)


# ### 52. Write a Python program to convert unix timestamp string to readable date.
# 
# Sample Unix timestamp string : 1284105682
#     
# Expected Output : 2010-09-10 13:31:22

# In[151]:


temp = int("1284105682")
print(dt.datetime.utcfromtimestamp(temp).strftime('%Y-%m-%d %H:%M:%S'))


# ### 53. Convert the below Series to pandas datetime : 
# 
# DoB = pd.Series(["07Sep59","01Jan55","15Dec47","11Jul42"])
# 
# Make sure that the year is 19XX not 20XX

# In[152]:


DoB = pd.Series(["07Sep59","01Jan55","15Dec47","11Jul42"])
dob = DoB.apply(lambda x: dt.datetime.strptime(x,"%d%b%y"))
dob = dob - pd.offsets.DateOffset(years=100)
print(dob)


# ### 54. Write a Python program to get days between two dates. 

# In[153]:


date1 = dt.date(2020, 2, 25)
date2 = dt.date(2019, 8, 25)
diffrence = date1 - date2
print(diffrence.days)


# ### 55. Convert the below date to datetime and then change its display format using the .dt module
# 
# Date = "15Dec1989"
# 
# Result : "Friday, 15 Dec 98"

# In[154]:


date1 = "15Dec1989"
b_date = dt.datetime.strptime(date1,"%d%b%Y")
f_date = dt.datetime.strftime(b_date,"%A, %d %b %y")
f_date


# ## The below exercises (56-66) required to use wind.data file 

# ### About wind.data:
# 
# The data have been modified to contain some missing values, identified by NaN.  
# 
# 1. The data in 'wind.data' has the following format:
"""
Yr Mo Dy   RPT   VAL   ROS   KIL   SHA   BIR   DUB   CLA   MUL   CLO   BEL   MAL
61  1  1 15.04 14.96 13.17  9.29   NaN  9.87 13.67 10.25 10.83 12.58 18.50 15.04
61  1  2 14.71   NaN 10.83  6.50 12.62  7.67 11.50 10.04  9.79  9.67 17.54 13.83
61  1  3 18.50 16.88 12.33 10.13 11.17  6.17 11.25   NaN  8.50  7.67 12.75 12.71
"""
The first three columns are year, month and day.  The remaining 12 columns are average windspeeds in knots at 12 locations in Ireland on that day. 
# # 56. Import the dataset wind.data and assign it to a variable called data and replace the first 3 columns by a proper date time index

# In[3]:


data = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//wind.data")
data["Date"] = pd.to_datetime(data[["Yr","Mo","Dy"]].astype(str).agg('-'.join, axis=1))
data = data.drop(columns=["Yr","Mo","Dy"])
data.head()


# ### 57. Year 2061 is seemingly imporoper. Convert every year which are < 70 to 19XX instead of 20XX.

# In[16]:


data["Date"] = np.where(pd.DatetimeIndex(data["Date"]).year < 2000,data.Date,data.Date - pd.offsets.DateOffset(years=100))
data.head()


# ### 58. Set the right dates as the index. Pay attention at the data type, it should be datetime64[ns].

# In[17]:


newData = data.set_index("Date")
newData.index.astype("datetime64[ns]")


# ### 59. Compute how many values are missing for each location over the entire record.  
# #### They should be ignored in all calculations below. 

# In[18]:


print(newData.isnull().values.ravel().sum())


# ### 60. Compute how many non-missing values there are in total.

# In[19]:


newData.count().sum()


# ### 61. Calculate the mean windspeeds over all the locations and all the times.
# #### A single number for the entire dataset.

# In[20]:


newData.mean().mean()


# ### 62. Create a DataFrame called loc_stats and calculate the min, max and mean windspeeds and standard deviations of the windspeeds at each location over all the days 
# 
# #### A different set of numbers for each location.

# In[21]:


def stats(x):
    x = pd.Series(x)
    Min = x.min()
    Max = x.max()
    Mean = x.mean()
    Std = x.std()
    res = [Min,Max,Mean,Std]
    indx = ["Min","Max","Mean","Std"]
    res = pd.Series(res,index=indx)
    return res
loc_stats = newData.apply(stats)
loc_stats


# ### 63. Create a DataFrame called day_stats and calculate the min, max and mean windspeed and standard deviations of the windspeeds across all the locations at each day.
# 
# #### A different set of numbers for each day.

# In[22]:


day_stats = newData.apply(stats,axis=1)
day_stats.head()


# ### 64. Find the average windspeed in January for each location.  
# #### Treat January 1961 and January 1962 both as January.

# In[23]:


january_data = newData[newData.index.month == 1]
print ("January windspeeds:")
print (january_data.mean())


# ### 65. Calculate the mean windspeed for each month in the dataset.  
# #### Treat January 1961 and January 1962 as *different* months.
# #### (hint: first find a  way to create an identifier unique for each month.)

# In[24]:


newdata = newData.groupby(lambda d: (d.month, d.year))
print ("Mean wind speed for each month in each location")
print (newdata.mean())


# ### 66. Calculate the min, max and mean windspeeds and standard deviations of the windspeeds across all locations for each week (assume that the first week starts on January 2 1961) for the first 52 weeks.

# In[25]:


first_year = newData[newData.index.year == 1961]
stats1 = newData.resample('W').mean().apply(lambda x: x.describe())
print (stats1)


# ## The below exercises (67-70) required to use appl_1980_2014.csv  file

# In[ ]:





# ### 67. Import the file appl_1980_2014.csv and assign it to a variable called 'apple'

# In[26]:


apple = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//appl_1980_2014.csv")


# In[27]:


apple.head()


# ### 68.  Check out the type of the columns

# In[28]:


apple.dtypes


# ### 69. Transform the Date column as a datetime type

# In[29]:


apple.Date = pd.to_datetime(apple.Date)


# In[30]:


apple.dtypes


# ### 70.  Set the date as the index

# In[31]:


apple.set_index("Date")


# ### 71.  Is there any duplicate dates?

# In[32]:


x = apple[apple.duplicated("Date")]
if len(x) != 0:
    print("Yes there are duplicates in date column")
else:
    print("No there are no duplicates in date column")


# ### 72.  The index is from the most recent date. Sort the data so that the first entry is the oldest date.

# In[33]:


apple = apple.sort_values(by="Date",ascending=True).reset_index(drop=True)
apple


# ### 73. Get the last business day of each month

# In[34]:


apple["month"] = pd.DatetimeIndex(apple.Date).month
apple["Date_wo"] = pd.DatetimeIndex(apple.Date).day
apple.groupby(by="month")[["Date_wo"]].max().reset_index()


# ### 74.  What is the difference in days between the first day and the oldest

# In[35]:


diffrence = apple.Date.max() - apple.Date.min() 
str(diffrence)


# ### 75.  How many months in the data we have?

# In[36]:


months_data = apple["month"].count()
months_data


# ## Section-8: The purpose of the below exercises is to understand how to create basic graphs

# ### 76. Plot the 'Adj Close' value. Set the size of the figure to 13.5 x 9 inches

# In[37]:


plt.figure(figsize=(13.5, 9))
plt.hist(apple["Adj Close"])
plt.show()


# ## The below exercises (77-80) required to use Online_Retail.csv file

# ### 77. Import the dataset from this Online_Retail.csv and assign it to a variable called online_rt

# In[40]:


online_rt = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//Online_Retail.csv", encoding= 'unicode_escape')


# In[41]:


online_rt.head()


# ### 78. Create a barchart with the 10 countries that have the most 'Quantity' ordered except UK

# In[42]:


mostQnty = online_rt.groupby(by="Country")[["Quantity"]].max().add_prefix("Max_")
mostQnty = mostQnty.sort_values(by="Max_Quantity",ascending=False).reset_index()


# In[43]:


mostQnty.drop(mostQnty[mostQnty.Country == "United Kingdom"].index,inplace=True)
mostQnty = mostQnty.drop(mostQnty.index[10:]).reset_index(drop=True)


# In[44]:


plt.figure(figsize=(12, 5))
sns.barplot(x="Country",y="Max_Quantity",data=mostQnty)
plt.xticks(rotation=25)
plt.show()


# ### 79.  Exclude negative Quatity entries

# In[45]:


online_rt1 = online_rt[(online_rt.Quantity > 0)].reset_index(drop=True)


# In[47]:


online_rt1.head()


# ### 80. Create a scatterplot with the Quantity per UnitPrice by CustomerID for the top 3 Countries
# Hint: First we need to find top-3 countries based on revenue, then create scater plot between Quantity and Unitprice for each country separately
# 

# In[48]:


customers = online_rt.groupby(['CustomerID','Country']).sum()

# there is an outlier with negative price
customers = customers[customers.UnitPrice > 0]

# get the value of the index and put in the column Country
customers['Country'] = customers.index.get_level_values(1)

# top three countries
top_countries =  ['Netherlands', 'EIRE', 'Germany']

# filter the dataframe to just select ones in the top_countries
customers = customers[customers['Country'].isin(top_countries)]

# Graph Section 

g = sns.FacetGrid(customers, col="Country")

g.map(plt.scatter, "Quantity", "UnitPrice", alpha=1)

# adds legend
g.add_legend()


# ## The below exercises (81-90) required to use FMCG_Company_Data_2019.csv file

# ### 81. Import the dataset FMCG_Company_Data_2019.csv and assign it to a variable called company_data

# In[51]:


company_data = pd.read_csv("C://Users//harsh//OneDrive//Documents//Python Case Studies//2. Basic Data Manipulation - Visualization Exercise//Exercise Data Files//FMCG_Company_Data_2019.csv")


# In[52]:


company_data.head()


# ### 82. Create line chart for Total Revenue of all months with following properties
# - X label name = Month
# - Y label name = Total Revenue

# In[53]:


plt.figure(figsize=(10,6))
plt.plot(company_data.Month,company_data.Total_Revenue,marker='o')
plt.ticklabel_format(style='plain',axis='y')
plt.xlabel('Month')
plt.ylabel('Total Revenue')
plt.show()


# ### 83. Create line chart for Total Units of all months with following properties
# - X label name = Month
# - Y label name = Total Units
# - Line Style dotted and Line-color should be red
# - Show legend at the lower right location.

# In[54]:


plt.figure(figsize=(10,6))
plt.plot(company_data.Month,company_data.Total_Units,label='Total Units for each month',linestyle='dotted',color='r',marker='o')
plt.xlabel('Month')
plt.ylabel('Total Units')
plt.legend(loc='lower right')
plt.show()


# ### 84. Read all product sales data (Facecream, FaceWash, Toothpaste, Soap, Shampo, Moisturizer) and show it  using a multiline plot
# - Display the number of units sold per month for each product using multiline plots. (i.e., Separate Plotline for each product ).

# In[55]:


plt.figure(figsize=(15,6))
plt.plot(company_data.Month,company_data.FaceCream,label='FaceCream',marker='o')
plt.plot(company_data.Month,company_data.FaceWash,label='FaceWash',marker='o')
plt.plot(company_data.Month,company_data.ToothPaste,label='Toothpaste',marker='o')
plt.plot(company_data.Month,company_data.Soap,label='Soap',marker='o')
plt.plot(company_data.Month,company_data.Shampo,label='Shampo',marker='o')
plt.plot(company_data.Month,company_data.Moisturizer,label='Moisturizer',marker='o')
plt.xlabel('Month')
plt.ylabel('Number of units sold')
plt.legend(loc='upper left')
plt.show()


# ### 85. Create Bar Chart for soap of all months and Save the chart in folder

# In[ ]:


plt.figure(figsize=(10,6))
sns.barplot(company_data.Month,company_data.Soap)
plt.ylabel('soap units sold')
plt.savefig('bar chart')
plt.show()


# ### 86. Create Stacked Bar Chart for Soap, Shampo, ToothPaste for each month
# The bar chart should display the number of units sold per month for each product. Add a separate bar for each product in the same chart.

# In[61]:


a=pd.pivot_table(data=company_data,index='Month',values=['Soap','Shampo','ToothPaste']).plot(kind='bar',stacked=True,figsize=(15,6))
for i in a.containers:
    a.bar_label(i,size=15,label_type='center')
plt.legend(bbox_to_anchor=(1.12,1))


# ### 87. Create Histogram for Total Revenue

# In[62]:


plt.hist(company_data.Total_Revenue,bins=20)
plt.xlabel('Total Revenue')
plt.ylabel('frequency')
plt.show


# ### 88. Calculate total sales data (quantity) for 2019 for each product and show it using a Pie chart. Understand percentage contribution from each product

# In[63]:


pie_chart=company_data.loc[:,'FaceCream':'Moisturizer'].sum()
plt.pie(pie_chart,labels=pie_chart.index,autopct='%.2f %%')
plt.title('total sales for each product')
plt.show()


# ### 89. Create line plots for Soap & Facewash of all months in a single plot using Subplot

# In[64]:


f, axes = plt.subplots(1,2,figsize=(20,5))
axes[0].plot(company_data.Month,company_data.Soap,marker='o')
axes[0].set_title('Soap sold per month ')
axes[1].plot(company_data.Month,company_data.FaceWash,marker='o')
axes[1].set_title('FaceWash sold per month ')
plt.show()


# ### 90. Create Box Plot for Total Profit variable

# In[65]:


company_data.Total_Profit.plot(kind='box')


# In[ ]:




